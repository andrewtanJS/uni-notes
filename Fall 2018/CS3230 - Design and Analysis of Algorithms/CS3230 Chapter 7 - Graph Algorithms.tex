\documentclass[a4paper]{article}

\input{header}

\title{%
	CS3230 Chapter 7 - Graph Algorithms \\
	\large Based on lectures by Chang Ee-Chien
	\\ Notes taken by Andrew Tan
	\\ AY18/19 Semester 1
	\\ }

\author{}
\date{\vspace{-5ex}}

\begin{document}
\maketitle

\begin{center}\begin{minipage}[c]{0.9\textwidth}\centering\footnotesize These notes are not endorsed by the lecturers, and I have modified them (often significantly) after lectures. They are nowhere near accurate representations of what was actually lectured, and in particular, all errors are almost surely mine.\end{minipage}\end{center}

\section{Graph representations}
There are several ways that we can represent graphs within a program:\\\\
Representation 1 - \textbf{Adjacency matrix}: each entry within the matrix is a 0 or a 1, and an edge from $a$ to $b$ is represented by filling in the entry at row $a$ and column $b$ with 1.\\
Representation 2 - \textbf{Adjacency list}: we use an array to deonte our list of vertices, and each entry $x$ in the array points to a linked list that contains the vertices such that there exists an edge from $x$ to that vertex.\\\\
If $|V| = n$ and $|E| = m$ where $V$ is our number of vertices and $E$ our number of edges, then our adjacency matrix will always have a space complexity of $O(n^2)$, while our adjacency list will have a space complexity of $O(m+n)$.

\subsection{Graph definitions}
We introduce a few graph definitions:
\begin{itemize}
	\item An \textbf{Undirected and weighted graph} is a graph where each edge is associated with a weight. Formally, each edge is of the form (\{$u, v$\}, $w$) where $u, v \in V$ and $w$ is a real number. We can easily express this graph with an incidence matrix by simply modifying our adjacency matrix to have entries corresponding to the weights.
	\item An \textbf{Euclidean graph} is a weighted undirected graph where each vertex is a point in $\R^2$. The weight of the edge \{$v,w$\} is the distance between $v$ and $w$, i.e. $||v-w||_2$. This is essentially a complete graph, and an Euclidean graph has a space complexity of $O(n)$ as all the edges are implicitly defined.
	\item A \textbf{subgraph} $G'=(V',E')$ is a subgraph of $G=(V,E)$ if $V'\subseteq V$ and $E'\subseteq E$.
	\item A \textbf{spanning tree} $T=(V',E')$ of $G=(V,E)$ is where $T$ is a subgraph of $G$, and
	\begin{itemize}
		\item $T$ is a tree
		\item $V' = V$
	\end{itemize}
	\item A \textbf{minimum spanning tree} $T$ of $G$ is where $T$ is the spanning tree of $G$ with minimum weight. If the edges of a graph are unique, then the minimum spanning tree is also unique.
	\item A \textbf{shortest path} from $v$ to $w$ is the path with minimum weight.
\end{itemize}

\section{Depth-First Search (DFS)}
Depth-first search, along with Breadth-first search, is one of the simplest algorithms for searching a graph. With DFS, we simply search "deeper" in the graph whenever possible.\\\\
Whenever DFS discovers a vertex $v$ during a scan of the adjacency list of an already discovered vertex $u$, it records this event by setting $v$'s predecessor attribute $v.\pi$ to $u$. We define the \textbf{predecessor subgraph} as such: we let $G_\pi = (V, E_\pi)$, where $E_\pi = \{(v, \pi, v): v\in V$ and $v.\pi \neq NIL\}$.\\
The predecessor subgraph of a DFS forms a depth-first forest comprising of several depth-first trees. The edges in $E_\pi$ are tree edges.\\\\
DFS colors vertices during the search to inicate their state. Each vertex is initially white, is grayed when it is discovered in the search, and is blackened when it is finished, that is, when its adjacency list has been examined completely.
\begin{lstlisting}[escapeinside={(*}{*)}]
DFS((*$G$*)):
for each vertex (*$u \in G.V$*):
	(*$u.color = WHITE$*)
	(*$u.\pi = NIL$*)
time = 0
for each vertex (*$u \in G.V$*):
	if (*$u.color == WHITE$*):
		DFS-VISIT((*$G, u$*))
\end{lstlisting}
\begin{lstlisting}[escapeinside={(*}{*)}]
DFS-Visit((*$G, u$*)):
(*$u.color = GRAY$*)
for each (*$v \in G.Adj[u]$*):
	if (*$v.color == WHITE$*):
		(*$v.\pi = u$*)
		DFS-Visit((*$G, u$*))
(*$u.color = BLACK$*)
\end{lstlisting}
The running time of the loops in DFS is in $\Theta(V)$ (exclusive of the time to execute calls to DFS-Visit). During an execution of DFS-Visit($G$, $v$), the loop in DFS-Visit executes $|Adj[v]|$ times. Since $\sum\limits_{v\in V}^{ }|Adj[v]| = \Theta(E)$, the total code of executing DFS-Visit is $\Theta(E)$. Thus, the running time of DFS is therefore $\Theta(V + E)$.

\section{Minimum spanning trees}
There are two key greedy algorithms for finding minimum spanning trees (MST): Kruskal's algorithm, and Prim's algorithm. We shall explore both of them, while also examining how the different graph representations affect their running time.
\subsection{Kruskal's Algorithm}
We shall sketch Kruskal's algorithm as such:
\begin{enumerate}
	\item Let $T$ be the empty set
	\item Repeat
	\begin{enumerate}
		\item Choose the minimum weighted edge $e$ such that $T + \{e\}$ does not contain a cycle
		\item Insert $e$ into $T$.
	\end{enumerate}
	Until $T$ is a spanning tree.
\end{enumerate}
Let $G=(V,E)$ be a connected, weighted, and undirected graph, let $T$ be any promising set of edges, and let $e$ be the minimum weighted edge such that $T + \{e\}$ does not contain a cycle.\\\\
For simplicity, we'll assume the weights of edges are distinct. The proof can easily be extended towards the general case. Let $U$ be the minimum spanning tree of $G$ such that $T \subset U$. If $e \in U$, then there is nothing to prove. Otherwise, $U + \{e\}$ will contain exactly one cycle. In this cycle, there is another edge $e'$ in $U$ that is not in $T$. Consider the subgraph $U + \{e\} - \{e'\}$. This subgraph $U'$ is a spanning tree. Since $T\subset U$, $T + \{e'\}$ does not contain a cycle.\\\\
By definition, $U$ is a MST and thus the weight of $U'$ must be larger or equal to that of $U$. This implies that the weight of $e'$ is smaller or equal to the weight of $e$. However, since the weights are distinct, the weight of $e'$ must be smaller than the weight of $e$. This contradicts the fact that $e$ is the minimum weighted edge that doesn't create a cycle.\\\\
Thus, we must have $e\in U$, and thus $T + \{e\}$ is promising.

\subsubsection{Implementations}
\textbf{DFS}:\\
We can use a DFS to check if $\{e\} + T$ contains a cycle, before inserting it into $T$.
\begin{lstlisting}[escapeinside={(*}{*)}]
Kruskal-DFS((*$G, w$*)):
(*$T = \emptyset$*)
sort the edges of (*$G.E$*) into nodecreasing order by weight (*$w$*)
for each edge (*$(u,v) \in G.E$*), taken in nondecreasing order by weight:
	using DFS, check if (*$\{(u,v)\}$*) contain a cycle, if there is no cycle, insert the edge (*$(u,v)$*) into (*$T$*).
return (*$T$*)
\end{lstlisting}
Sorting will simply take $|E| log |E|$. Here, the running time of DFS is in $\Theta(|V|)$, since the number of edges in a tree can't be more than $|V| - 1$. In the worst case, the number of DFS calls is in $\Theta(|E|)$, so the running time is in $\Theta(|E||V|)$.\\\\
\textbf{Disjoint Set}:\\
A disjoint set data structure maintains a collection of disjoint sets \{$S_1, S_2, \dots, S_k$\}. Each set $S_i = \{a_1,a_2,\dots,a_m\}$ is identified by a representative which can be any element in the set.\\\\
The data structure supports three operations:
\begin{enumerate}
	\item Create-Set($x$): Creates a set \{$x$\}.
	\item Find-Set($x$): Finds the set containing $x$ and returns the representative of this set.
	\item Union($x, y$): Merges the two sets which contain $x$ and $y$ respectively and returns the representative of this combined set.
\end{enumerate}
We can implement this with a data-structure known as union by rank with path compression.\\\\
Back to Kruskal's algorithm, we can use our disjoint set to implement this algorithm:
\begin{lstlisting}[escapeinside={(*}{*)}]
Kruskal-Disjoint-Set((*$G, w$*)):
(*$T = \emptyset$*)
for each vertex (*$v \in G.V$*):
	Make-Set((*$v$*))
sort the edges of (*$G.E$*) into nodecreasing order by weight (*$w$*)
for each edge (*$(u,v) \in G.E$*), taken in nondecreasing order by weight:
	if Find-Set((*$u$*)) (*$\neq$*) Find-Set((*$v$*)):
		(*$T = T\cup \{(u,v)\}$*)
		Union((*$u,v$*))
return (*$T$*)
\end{lstlisting}
With our disjoint set, the running time of $m$ operations on $n$ elements is in $O(m\alpha(n))$, where $n$ is the number of elements in the data-structure, and $\alpha(n)$ is an extremely slow-growing function ($\alpha(n) \approx log^*n$).\\\\
Using this, our running time is improved accordingly, with sorting taking $\Theta(|E|log|E|)$, and our number of set operations are less than $3|E|$. Since $3|E|\alpha(|V|) \in O(|E|log|E|)$, our runnning time of Kruskal's algorithm is now $\Theta(|E|log|E|)$.

\subsection{Prim's Algorithm}
We shall sketch Prim's algorithm as such:
\begin{enumerate}
	\item Let $T$ be the empty set
	\item Let $B$ be the set {$v_0$} where $v_0$ can be any vertex.
	\item Repeat
	\begin{enumerate}
		\item Choose the minimum weighted edge $e=\{u,v\}$ such that $u\in B$ and $\{e\} + T$ is a tree.
		\item Insert $e$ into $T$.
		\item Insert $v$ into $B$.
	\end{enumerate}
	Until $T$ is a spanning tree.
\end{enumerate}
Let $G=(V,E)$ be a connected, weighted, and undirected graph, let $T$ be any promising set of edges and $T$ is connected, and let $e$ be the shortest edge such that $T + \{e\}$ is a connected tree.\\\\
We'll continue to assume that the weights of edges are distinct. The proof can be extended easily to the general case. Let $U$ be the MST of $G$. Thus we have $T\subset U$. Let $B$ be the vertices that are in $T$. Suppose an edge has one vertex in $B$, we say that the edge touches $B$. If $e \in U$, then there is nothing to prove. Otherwise, $U + \{e\}$ will contain exactly one cycle. In this cycle, there is another edge $e'$ that is not in $T $, and $e'$ touches $B$. Since $e'$ touches $B$, the subgraph $T + \{e'\}$ is a tree. Consider the subgraph $U + \{e\} - \{e'\}$. This subgraph $U'$ is a spanning tree.\\\\
Since $U$ is the MST, the weight of $U'$ must be larger or equal to that of $U$. This implies that the weight of $e'$ is smaller than the weight of $e$. This contradicts the fact that $e$ is the smallest edge that touches $B$.\\\\
Thus, we must have $e\in U$, and thus $T + \{e\}$ is promising.

\subsubsection{Implementation}
\textbf{Storing edges in a heap}:\\
A sketch of the algorithm is as follows:
\begin{enumerate} \itemsep0em
	\item Let $T$ be the empty set, and let $B$ be an empty heap
	\item Let $B$ be the set {$v$}, where $v$ is any vertex in $V$
	\item For each $u$ adjacent to $v$, insert the weight of ${v,u}$ into the heap (the directed edge ($v,u$ ) is stored as the data)
	\item Repeat
	\begin{enumerate}
		\item Extract the minimum weight from the heap. Let $(p,q)$ be the edge associated with the minimum weight (Note that $p$ is guaranteed to be in the set $B$)
		\item If $q\notin B$, \\then insert $q$ into $B$, insert \{$p,q$\} into $T$, and for each $u$ which is adjacent to $q$ and $u\notin B$, insert the weight of \{$q,u$\} into the heap. The associated data is the directed edge ($q,u$)
	\end{enumerate}
	Until $|B| == |V|$
\end{enumerate}
There are at most $|E|$ calls of extract\_min and insertion, hence our running time is $O(|V| + |E|log|E|)$.\\\\
\textbf{Storing vertices in a heap}:\\
We can achieve a smaller time complexity by storing the vertices rather than the edges in the heap. We will need an additional operation with the heap: reduce\_key($A, k, x$), where $A$ is the array storing the heap, $k$ is the indice of a vertex in the array, and $x$ is the value we wish to reduce $A[k]$ to. The reduced value must be smaller or equal to the original value, as we shall reheapify our heap with a call to SiftUp($A, k$).\\\\
Below is a sketch of the algorithm:
\begin{enumerate} \itemsep0em
	\item Let $T$ be the empty set
	\item Let $H$ be an empty heap
	\item For each $v\in V$, let $k[v]$ = $\infty$, and let $n[v]$ = \{$v,v$\}
	\begin{enumerate}
		\item Find the vertex $p$ with the minimum key
		\item Insert $p$ into $B$.
		\item Insert $n[p]$ into $T$.
		\item For each $u$ such that \{$p,u$\} $\in E$,
		if the weight of {$p,u$ is less than $k[u]$}, then reduce $k[u]$ to the weight of \{$p,u$\}, and let $n[u] = \{p,u\}$.
	\end{enumerate}
	Until $|B| == |V|$
\end{enumerate}
We call extract\_min a total of $|V|$ times, and reduce\_key at most $|E|$ times. Thus, our running time is in $O(|V|log|V| + |E|log|V|) = O(|E| log|V|)$, which is the same as our disjoint set implementation of Kruskal's algorithm (since the graph is connected, $log|E| \in \Theta(log|V|)$).\\\\
We can in fact improve our vertices implementation of Prim's algorithm using Fibonacci Heaps. A Fibbonacci heap supports the following operations:
\begin{itemize}
	\itemsep0em
	\item extract\_min: $O(log(n))$ time
	\item insert: $\Theta(1)$ amortized time
	\item reduce\_key: $\Theta(1)$ amortized time
\end{itemize}
The discussion on amortized time is in a later chapter, but for now it can be thought of as roughly the average time of each operation in a sequence of operations.\\\\
Using the Fibonacci heaps, our vertices implementation for Prim's algorithm runs in $O(|V|log|V|+|E|)$ time.

\section{Single-Source Shortest-Path}
Given a weighted directed graph $G = (V,E)$ and $s \in V$, we want to find the shortest path from $s$ to every vertex $v\in V$. We shall explore Dijkstra's algorithm to solve the problem in the case where the graph contains only edges of non-negative weights.\\\\
Before we get started on Dijkstra's algorithm, we shall introduce a few helpful functions and terminology. For each vertex $v\in V$, we maintain an attribute $v.d$ which is an upper bound on the weight of a shortest path from source $s$ to $v$. We call $v.d$ a shortest-path estimate. We can initialize the shortest-path estimates accordingly:
\begin{lstlisting}[escapeinside={(*}{*)}]
Initialize-Single-Source((*$G, s$*)):
for each vertex (*$v \in G.V$*):
	(*$v.d = \infty$*)
	(*$v.\pi = NIL$*)
(*$s.d = 0$*)
\end{lstlisting}
Initializing the shortest path estimates and predecessors with this algorithm takes $\Theta(V)$ time.\\\\
The process of \textbf{relaxing} an edge ($u,v$) consists of testing whether we can improve the shortest path to $v$ found so far by going through $u$ and, if so, updating $v.d$ and $v.\pi$.
\begin{lstlisting}[escapeinside={(*}{*)}]
Relax((*$u, v, w$*)):
if (*$v.d > u.d + w(u,v)$*):
	(*$v.d = u.d + w(u,v)$*)
	(*$v.\pi = u$*)
\end{lstlisting}
The above code performs a relaxation step on edge ($u,v$) in $O(1)$ time.\\\\
Assuming all edge weights are non-negative, we can thus use \textbf{Dijkstra's algorithm} to find the single-source shortest-path.
\begin{lstlisting}[escapeinside={(*}{*)}]
Dijkstra((*$G, w, s$*)):
Initialize-Single-Source((*$G,s$*))
(*$S=\emptyset$*)
(*$Q=G.V$*)
while (*$Q\neq \emptyset$*):
(*$u$*) = Extract-Min((*$Q$*))
(*$S = S\cup \{u\}$*)
for each vertex (*$v \in G.Adj[u]$*):
Relax((*$u,v,w$*))
\end{lstlisting}
Notice that Dijkstra's algorithm resembles Prim's algorithm. Likewise, the running time for Dijkstra's algorithm is the same and is in $\Theta(|E| + |V| log(|V|))$.

\end{document}